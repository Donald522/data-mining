{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 4. Конструирование текстовых признаков из твитов пользователей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый этап - сбор твитов пользователей. Необходимо подключаться к Twitter API и запрашивать твиты по id пользователя. \n",
    "Подключение к API подробно описано в ДЗ 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import twitter\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CONSUMER_KEY = \"PTNcJUZmk5w75LEQsSraCuKo1\"\n",
    "CONSUMER_SECRET = \"1NoB3ANMKEDrlabb5C1b3oBdminESRsTrgfKR6dAjhOHPx4oyo\"\n",
    "\n",
    "ACCESS_TOKEN_KEY = \"703693954907299840-hhW6FX4nmKnECqsLT4CP7v9EG6rE8FA\"\n",
    "ACCESS_TOKEN_SECRET = \"sIQ9JTCnCxfUvBd2pwFvpTbERExLHkmJPGcch4hiPv7Km\"\n",
    "\n",
    "api = twitter.Api(consumer_key=CONSUMER_KEY, \n",
    "                  consumer_secret=CONSUMER_SECRET, \n",
    "                  access_token_key=ACCESS_TOKEN_KEY, \n",
    "                  access_token_secret=ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения твитов пользователя может быть использован метод GetUserTimeline из библиотеки python-twitter. Он позволяет получить не более 200 твитов пользователя. По каждому пользователю достаточно собрать 200 твитов.\n",
    "\n",
    "Метод имеет ограничение по количеству запросов в секунду. Для получения информации о промежутке времени, которое необходимо подождать для повторного обращения к API может быть использован метод `GetSleepTime`. Для получения информации об ограничениях запросов с помощью метода `GetUserTimeLine` необходимо вызывать `GetSleepTime` с параметром \"statuses/user_timeline\".\n",
    "\n",
    "Метод GetUserTimeline возвращает объекты типа Status. У этих объектов есть метод AsDict, который позволяет представить твит в виде словаря.\n",
    "\n",
    "Id пользователей необходимо считать из файла, как было сделано в ДЗ 1.\n",
    "\n",
    "Необходимо реализовать функцию `get_user_tweets(user_id)`. Входной параметр - id пользователя из файла. Возвращаемое значение - массив твитов пользователя, где каждый твит представлен в виде словаря. Предполагается, что информация о пользователе содержится в твитах, которые пользователь написал сам. Это означает, что можно попробовать отфильтровать ответы другим пользователям, ссылки и ретвиты, а так же картинки и видео, так как наша цель - найти текстовую информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_user_tweets(user_id):\n",
    "    \"\"\"returns list of tweets as dicts\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            delay = api.GetSleepTime(\"statuses/user_timeline\")\n",
    "            if delay > 0:\n",
    "                print 'delay...'\n",
    "                time.sleep(delay)\n",
    "            try:    \n",
    "                statuses = api.GetUserTimeline(user_id, count = 200, include_rts = False, exclude_replies = True, trim_user=True)\n",
    "                for tweet in statuses:\n",
    "                    tweet.text = re.sub(r'(?:\\@|https?\\://)\\S+', '', tweet.text)\n",
    "                    tweet.text = re.sub(r'[^\\w]', ' ', tweet.text)\n",
    "                return [tweet.AsDict() for tweet in statuses if 'media' not in tweet.AsDict().keys()]\n",
    "            except:\n",
    "                return [{'user':{'id':user_id}}]\n",
    "        except twitter.TwitterError:\n",
    "            print 'delay....'\n",
    "            time.sleep(3*60)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lang': u'en', 'favorited': False, 'truncated': False, 'text': u' Well  um  actually a pretty nice little Saturday  We re going to go to Home Depot     Maybe  ', 'created_at': u'Sat Apr 16 20:31:28 +0000 2016', 'retweeted': False, 'source': u'<a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>', 'user': {'id': 30005269}, 'urls': {u'https://t.co/4WtAk3MaYb': u'https://www.instagram.com/p/BERk6UnCvaL/'}, 'id': 721435864774111232}\n"
     ]
    }
   ],
   "source": [
    "print (get_user_tweets(30005269))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбор текста твита"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка текста предполагает разбиение текста на отдельные элементы - параграфы, предложения, слова. Мы будем преобразовывать текст твита к словам. Для этого текст необходимо разбить на слова. Сделать это можно, например, с помощью регулярного выражения.\n",
    "\n",
    "Необходимо реализовать функцию, `get_words(text)`. Входной параметр - строка с текстом. Возвращаемое значение - массив строк (слов). Обратите внимание, что нужно учесть возможное наличие пунктуации и выделить по возможности только слова. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    \"\"\"returns list of words\"\"\"\n",
    "    return [word.strip(string.punctuation) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print get_words(\"Here are different words: one, two, three!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее полученные слова необходимо привести к нормальной форме. То есть привести их к форме единственного числа настоящего времени и пр. Сделать это можно с помощью библиотеки nltk. Информацию по загрузке, установке библиотеки и примерах использования можно найти на сайте http://www.nltk.org/\n",
    "\n",
    "Для загрузки всех необходимых словарей можно воспользоваться методом download из библиотеки nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дальнейшей обработки слова должны быть приведены к нижнему регистру. \n",
    "\n",
    "Для приведения к нормальной форме можно использовать `WordNetLemmatizer` из библиотеки nltk. У этого класса есть метод `lemmatize`.\n",
    "\n",
    "Также необходимо убрать из текста так называемые стоп-слова. Это часто используемые слова, не несущие смысловой нагрузки для наших задач. Сделать это можно с помощью `stopwords` из nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `get_tokens(words)`. Входной параметр - массив слов. Возвращаемое значение - массив токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokens(words):\n",
    "    \"\"\"returns list of tokens\"\"\"\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return [wnl.lemmatize(word.lower(), 'v') for word in words if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print get_tokens([\"tested\", \"function\", \"and\", \"using\", \"a\", \"lot\", \"of\", \"Different\", \"words\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `get_tweet_tokens(tweet)`. Входной параметр - текст твита. Возвращаемое значение -- токены твита. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tweet_tokens(tweet):\n",
    "    words = get_words(tweet)\n",
    "    return get_tokens(words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'hey',\n",
       " u'usu',\n",
       " u'students',\n",
       " u'we',\n",
       " u'debate',\n",
       " u'watch',\n",
       " u'party',\n",
       " u'right',\n",
       " u'old',\n",
       " u'main',\n",
       " u'115',\n",
       " u'come',\n",
       " u'free',\n",
       " u'pop',\n",
       " u'popcorn',\n",
       " u'intellectual',\n",
       " u'debate']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tweet_tokens((get_user_tweets(758669826))[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `collect_users_tokens()`. Функция должна сконструировать матрицу признаков пользователей. В этой матрице строка - пользователь. Столбец - токен. На пересечении - сколько раз токен встречается у пользователя.\n",
    "Для построения матрицы можно использовать `DictVectorizer` из `sklearn.feature_extraction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_users_tokens(df_users):\n",
    "    \"\"\"returns users list and list of user dicts. Each dict contains frequence of user tokens\"\"\"\n",
    "    \n",
    "    users_tokens = []\n",
    "    users = df_users['uid']\n",
    "    not_processed_users = []\n",
    "    for user in users:\n",
    "        if user not in processed_users:\n",
    "            not_processed_users.append(user)\n",
    "            \n",
    "    print 'users =', len(users)\n",
    "    print 'processed_users =', len(processed_users)\n",
    "    print 'not_processed_users =', len(not_processed_users)\n",
    "    \n",
    "    print 'start' \n",
    "    for i, user in enumerate(not_processed_users):\n",
    "        tweets_array = get_user_tweets(user)\n",
    "        if user in user_records.keys():\n",
    "            user_records[user] += tweets_array\n",
    "        else:\n",
    "            user_records[user] = tweets_array\n",
    "        if i % 100 == 0:\n",
    "            print 'doing...'\n",
    "        for tweet in tweets_array:\n",
    "            json.dump(tweet, f)\n",
    "            f.write('\\n')\n",
    "        break\n",
    "    print 'finish'\n",
    "    \n",
    "    for user, tweets in user_records.iteritems():\n",
    "        user_dict = {}\n",
    "        for tweet in tweets:\n",
    "            try:\n",
    "                tweet_tokens = get_tweet_tokens(tweet['text'])\n",
    "            except:\n",
    "                tweet_tokens = []\n",
    "            for token in tweet_tokens:\n",
    "                if token in user_dict.keys():\n",
    "                    user_dict[token] += tweet_tokens.count(token)\n",
    "                else:\n",
    "                    user_dict[token] = tweet_tokens.count(token)\n",
    "                    \n",
    "        if len(user_dict.keys()) > 0:         \n",
    "            users_tokens += [user_dict]\n",
    "        else:\n",
    "            users = users[users != user]\n",
    " \n",
    "    return list(users), users_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users = 6898\n",
      "processed_users = 29\n",
      "not_processed_users = 6869\n",
      "start\n",
      "doing..."
     ]
    }
   ],
   "source": [
    "TRAINING_SET_URL = \"twitter_train.txt\"\n",
    "EXAMPLE_SET_URL = \"twitter_example.txt\"\n",
    "df_users_train = pd.read_csv(TRAINING_SET_URL, sep=\",\", header=0, names=[\"uid\", \"cat\"])\n",
    "df_users_ex = pd.read_csv(EXAMPLE_SET_URL, sep=\",\", header=0, names=[\"uid\", \"cat\"])\n",
    "df_users_ex['cat'] = None\n",
    "df_users = pd.concat([df_users_train, df_users_ex])\n",
    "\n",
    "user_records = {}\n",
    "tmp_file_name = 'tmp_user_records'\n",
    "if os.path.exists(tmp_file_name):\n",
    "    with open(tmp_file_name) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = (json.loads(line))\n",
    "                uid = rec['user']['id']\n",
    "                if uid in user_records.keys():\n",
    "                    user_records[uid] += [rec]\n",
    "                else:\n",
    "                    user_records[uid] = [rec]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "processed_users = set()\n",
    "for r in user_records.keys():\n",
    "    try:\n",
    "        processed_users.add(r)\n",
    "    except:\n",
    "        print r\n",
    "        \n",
    "f = open(tmp_file_name, 'a')\n",
    "\n",
    "users, users_tokens = collect_users_tokens(df_users)\n",
    "print 'func end'\n",
    "v = DictVectorizer()\n",
    "vs = v.fit_transform(users_tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Сохраним полученные данные в файл. Используется метод savez из numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savez(\"files/out_4.dat\", data=vs, users=users, users_tokens=users_tokens )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее для получения представления о полученной информацию о токенах предлагается отобразить ее в виде облака тэгов. [Подсказка](http://anokhin.github.io/img/tag_cloud.png). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_tag_cloud(v, vs):\n",
    "    \"\"\"Draws tag cloud of found tokens\"\"\"\n",
    "    # your code here\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
